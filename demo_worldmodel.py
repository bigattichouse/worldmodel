#!/usr/bin/env python3
"""
WorldModel Demo - Shows current capabilities
"""
import sys
sys.path.insert(0, 'src')

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from src.training.dataGenerator import DataGenerator
from src.utils.config import TrainingConfig

def demo_current_capabilities():
    """Demonstrate what the WorldModel system can do right now."""
    
    print("ğŸŒ WorldModel System Demo")
    print("=" * 40)
    
    # 1. Training Data Infrastructure
    print("\nâœ… Training Infrastructure:")
    generator = DataGenerator(TrainingConfig())
    examples = generator.load_dataset('./data/worldmodel_final_training.json')
    print(f"   ğŸ“Š {len(examples)} training examples loaded")
    print(f"   ğŸ·ï¸  {len(set(ex.category for ex in examples))} categories")
    
    # Show a sample
    sample = examples[0]
    print(f"\nğŸ“ Sample Training Example:")
    print(f"   Input: {sample.input_text}")
    print(f"   Output: {sample.target_output[:80]}...")
    
    # 2. Model Inference (works!)
    print(f"\nâœ… Model Inference:")
    tokenizer = AutoTokenizer.from_pretrained('../model/phi-4-mini-instruct')
    model = AutoModelForCausalLM.from_pretrained('../model/phi-4-mini-instruct', torch_dtype=torch.float32)
    print(f"   ğŸ§  Phi-4-mini loaded successfully")
    print(f"   ğŸ’¾ Model size: 3.8B parameters")
    print(f"   ğŸ–¥ï¸  Device: {next(model.parameters()).device}")
    
    # 3. WorldModel Prompting
    print(f"\nğŸ§ª WorldModel Prompting Test:")
    
    prompt = """You are a WorldModel assistant. For computational tasks, use this format:

<think>reasoning here</think>
<model>
python_code_here
</model>
<requires>python:category</requires>

User: Calculate 15% tip on a $67.50 bill"""

    print("   ğŸ“ Testing prompt...")
    inputs = tokenizer(prompt, return_tensors='pt')
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            temperature=0.3,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    generated = response[len(prompt):].strip()
    
    print("   ğŸ¤– Generated response:")
    print(f"   {generated[:200]}...")
    
    # 4. Data Generation
    print(f"\nâœ… Data Generation:")
    print("   ğŸ­ Can generate new training examples")
    print("   ğŸ“ˆ Can expand dataset for future training")
    print("   ğŸ”§ Template system for various problem types")
    
    print(f"\nğŸ¯ Summary - What Works NOW:")
    print("   â€¢ WorldModel prompting (with examples)")
    print("   â€¢ Model inference on CPU")
    print("   â€¢ Training data creation and management")
    print("   â€¢ ROCm environment ready for GPU inference")
    print("   â€¢ Complete training infrastructure")
    
    print(f"\nâ³ What's Next:")
    print("   â€¢ Wait for transformers update (2-4 weeks)")
    print("   â€¢ Or switch to proven compatible model")
    print("   â€¢ Then fine-tune for instinctive behavior")
    print("   â€¢ Deploy with ROCm GPU acceleration")
    
    print(f"\nğŸš€ The foundation is solid!")

if __name__ == "__main__":
    demo_current_capabilities()