# WorldModel: LLM Training for Structured Reasoning

A comprehensive system that trains language models to perform computational tasks using structured reasoning. Instead of generating free-form text, trained models produce systematic responses with `<think>` tags for reasoning, `<model>` tags for executable code, and `<requires>` tags for dependencies. The system then safely executes the generated code and returns verified results, bridging natural language understanding with reliable computation.

This approach makes AI reasoning transparent, verifiable, and practically useful for mathematical calculations, data analysis, system tasks, and complex problem-solving.

## üß† How It Works

The system teaches models to generate structured responses that combine reasoning with executable code:

**Input**: "What's 15% of 200?"

**Model Output**:
```
<think>I need to calculate 15% of 200...</think>
<model>
result = 0.15 * 200
print(f"15% of 200 = {result}")
</model>
<requires>python:math</requires>

15% of 200 equals 30.
```

**System Response**:
1. **Parses** the structured output (`<think>`, `<model>`, `<requires>`)
2. **Executes** the generated code safely in a sandboxed environment
3. **Returns** results with explanation and verification

This creates AI that doesn't just "know" the answer, but can **show its work** and **prove its calculations** through executable code.

## üõ†Ô∏è Installation & Setup

### 1. Prerequisites
- **Python 3.8-3.12** (avoid Python 3.13)
- **ROCm 7.1+** (for AMD GPU training) or **CUDA 11.8+** (for NVIDIA)
- **16GB+ VRAM** (32GB+ recommended for optimal training)
- **20GB+ disk space** (for models and datasets)

### 2. Clone Repository
```bash
git clone <repository-url>
cd worldmodel
```

### 3. Create Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 4. Install Dependencies

**For AMD ROCm (Recommended)**:
```bash
# Install PyTorch with ROCm support
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0

# Install other dependencies
pip install -r requirements.txt
```

**For NVIDIA CUDA**:
```bash
# Install PyTorch with CUDA support
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install other dependencies
pip install -r requirements.txt
```

**For CPU-Only (Slower)**:
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install -r requirements.txt
```

### 5. Download Base Model
```bash
# Create model directory
mkdir -p ../model

# Download Qwen3-0.6B model (recommended for testing)
cd ../model
git clone https://huggingface.co/Qwen/Qwen3-0.6B

# Alternative: Download larger model for better performance
# git clone https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct
# git clone https://huggingface.co/Qwen/Qwen2.5-3B-Instruct

cd ../worldmodel
```

### 6. Set Environment Variables (AMD ROCm)
```bash
# For AMD ROCm users - add to ~/.bashrc or run before training
export HSA_OVERRIDE_GFX_VERSION=9.0.6  # For MI50
# export HSA_OVERRIDE_GFX_VERSION=10.3.0  # For newer AMD GPUs
export PYTORCH_ROCM_ARCH=gfx906  # For MI50
export HIP_VISIBLE_DEVICES=0
```

### 7. Verify Installation
```bash
# Test GPU detection
python3 -c "import torch; print(f'GPU Available: {torch.cuda.is_available()}'); print(f'GPU Name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\"}')"

# Quick ROCm test (AMD users)
python3 -c "import torch; x = torch.randn(100, 100).cuda(); print('ROCm working!') if torch.cuda.is_available() else print('Using CPU')"
```

## üöÄ Quick Start

### Training
```bash
# Complete workflow (training + inference test)
./complete_workflow.sh

# Training only  
python3 train_worldmodel_rocm.py
```

### Inference
```bash
# Interactive session (secure by default)
python3 run_worldmodel_inference.py --interactive

# Single query (secure by default)
python3 run_worldmodel_inference.py "Calculate 25% of 400"

# Direct execution mode (less secure)
python3 run_worldmodel_inference.py --no-sandbox "What's today's date?"
```

## üìÅ Structure

```
worldmodel/
‚îú‚îÄ‚îÄ train_worldmodel_rocm.py    # Main training script
‚îú‚îÄ‚îÄ run_worldmodel_inference.py # Inference engine with code execution
‚îú‚îÄ‚îÄ complete_workflow.sh        # One-command training + testing
‚îú‚îÄ‚îÄ requirements.txt            # Dependencies
‚îú‚îÄ‚îÄ data/                       # Training datasets (1000+ examples)
‚îú‚îÄ‚îÄ sandbox/                    # QEMU sandbox for secure code execution
‚îú‚îÄ‚îÄ docs/                       # Documentation and guides
‚îî‚îÄ‚îÄ archive/                    # Development scripts and variants
```

## üìä Training Data

**1000+ Examples Across**:
- **Math**: Basic arithmetic, percentages, geometry
- **Text Analysis**: Character counting, string processing
- **System Tasks**: Date/time, file operations, environment info
- **Data Processing**: Statistics, JSON/CSV parsing
- **Advanced Math**: Trigonometry, linear algebra, number theory

## üîí Security Features

**QEMU Sandbox Integration** (Enabled by Default):
- **Complete Isolation**: AI-generated code runs in QEMU virtual machines
- **No Host Impact**: Malicious or buggy code cannot affect your system
- **Resource Limits**: CPU, memory, and execution time constraints
- **Ephemeral Execution**: VMs reset after each code execution
- **Easy Setup**: One-command installation via git submodule
- **Automatic Fallback**: Falls back to direct execution if sandbox unavailable

```bash
# Set up secure sandbox (one-time setup)
cd sandbox && ./setup.sh

# Normal usage (secure by default)
python3 run_worldmodel_inference.py "Run any code safely"

# Disable sandbox if needed (not recommended)
python3 run_worldmodel_inference.py --no-sandbox "Direct execution"
```

## ‚öôÔ∏è Requirements

- **Python 3.8+**
- **ROCm 7.1+** (for AMD GPU training)
- **PyTorch 2.4.1+rocm6.0** (specific version for MI50 compatibility)
- **Transformers, PEFT, Accelerate**

## üéØ Training Results

- **Loss reduction**: 1.46 ‚Üí 0.59 (60% improvement in 3 epochs)
- **Training time**: 
  - **30 epochs**: ~6-10 hours (current default, comprehensive training)
  - **3 epochs**: ~6 minutes (quick test)
- **Structure quality**: 2/3 ‚Üí 3/3 WorldModel tags with extended training
- **GPU utilization**: 80-95% on AMD MI50 (32GB VRAM)

## üîß Troubleshooting

### Common Issues

**Model download fails**:
```bash
# Make sure you have git-lfs installed
sudo apt install git-lfs  # Ubuntu/Debian
brew install git-lfs     # macOS
git lfs install
```

**GPU not detected**:
- **AMD ROCm**: Check `rocm-smi` shows your GPU
- **NVIDIA**: Check `nvidia-smi` shows your GPU  
- **Both**: Verify environment variables are set

**Out of memory errors**:
- Reduce model size: Use Qwen3-0.6B instead of larger models
- Reduce batch size in training script
- Use gradient checkpointing (enabled by default)

**Training loss not decreasing**:
- Check that training data is properly formatted
- Verify model path is correct
- Ensure sufficient training epochs (30 recommended)

**ROCm-specific issues**:
- Use PyTorch 2.4.1+rocm6.0 (not newer versions)
- Set `HSA_OVERRIDE_GFX_VERSION=9.0.6` for MI50
- Check `docs/` for detailed ROCm troubleshooting

### Getting Help
- Check `docs/` directory for detailed guides
- Review training logs in `logs/` directory
- Test with CPU-only mode if GPU issues persist

## üìñ Documentation

See `docs/` directory for:
- **Training guides** and ROCm setup
- **Inference examples** and API reference  
- **Troubleshooting** for common issues
- **Performance optimization** tips

## üîß ROCm Compatibility

Optimized for **AMD Instinct MI50** with:
- **Native gfx906 architecture** (HSA_OVERRIDE_GFX_VERSION=9.0.6)
- **Conservative memory settings** for 32GB VRAM
- **Stable PyTorch 2.4.1+rocm6.0** (avoids newer version issues)

## üéâ Ready to Use

This system is production-ready for:
- **Educational tools** teaching step-by-step reasoning
- **Computational assistants** with code execution
- **Research platforms** for structured AI reasoning
- **Custom applications** requiring reliable code generation

The WorldModel approach bridges natural language understanding with executable computation, making AI reasoning transparent and verifiable.