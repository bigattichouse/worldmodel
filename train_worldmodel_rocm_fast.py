#!/usr/bin/env python3
"""
Optimized WorldModel Training for ROCm MI50
==========================================

High-performance version implementing priority optimizations:
1. FP16 precision (2x speedup)
2. Disabled gradient checkpointing (30-40% speedup) 
3. Increased batch size (50%+ speedup)
4. Optimized attention backend
5. Parallel data loading

Expected: 2-3x total speedup vs original version.
"""

import torch
import torch.nn as nn
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, TrainingArguments, 
    Trainer, DataCollatorForLanguageModeling, TrainerCallback
)
import json
import os
import time
from pathlib import Path
from typing import List, Dict, Optional
from dataclasses import dataclass
from torch.utils.data import Dataset
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ROCm Detection
print("=== WorldModel ROCm Training (OPTIMIZED) ===")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    device = torch.device("cuda")
    gpu_name = torch.cuda.get_device_name()
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"‚úÖ GPU: {gpu_name}")
    print(f"   Memory: {total_memory:.1f}GB")
    torch.cuda.empty_cache()
else:
    device = torch.device("cpu")
    print("‚ùå Using CPU")

print("=" * 60)

# OPTIMIZATION 4: Force SDPA, disable FlashAttention for ROCm stability
print("üöÄ Configuring attention backends for ROCm...")
torch.backends.cuda.enable_flash_sdp(False)
torch.backends.cuda.enable_mem_efficient_sdp(False)
# Enable standard SDPA (most reliable on ROCm)
torch.backends.cuda.enable_math_sdp(True)  
print("‚úÖ Attention: Using SDPA (ROCm-optimized)")

@dataclass
class WorldModelExample:
    user: str
    assistant: str

class PerformanceMonitor:
    """Monitor training performance metrics."""
    
    def __init__(self):
        self.iteration_times = []
        self.memory_usage = []
        self.start_time = None
        
    def start_iteration(self):
        self.start_time = time.time()
        
    def end_iteration(self, batch_size, seq_length):
        if self.start_time:
            iteration_time = time.time() - self.start_time
            self.iteration_times.append(iteration_time)
            
            if torch.cuda.is_available():
                memory_gb = torch.cuda.max_memory_allocated() / 1e9
                self.memory_usage.append(memory_gb)
            
            tokens_per_sec = (batch_size * seq_length) / iteration_time
            
            print(f"‚ö° Performance: {iteration_time:.2f}s/it, "
                  f"{tokens_per_sec:.0f} tok/s, "
                  f"mem: {memory_gb:.1f}GB")

class WorldModelDataset(Dataset):
    def __init__(self, examples: List[WorldModelExample], tokenizer, max_length: int = 400):  # OPTIMIZATION: Reduced max_length
        self.examples = examples
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        example = self.examples[idx]
        
        # Format as conversation
        text = f"User: {example.user}\nAssistant: {example.assistant}"
        
        # Tokenize
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt"
        )
        
        input_ids = encoding["input_ids"].flatten()
        attention_mask = encoding["attention_mask"].flatten()
        
        # Labels are the same as input_ids for causal LM
        labels = input_ids.clone()
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }

def parse_training_file(file_path: str) -> List[WorldModelExample]:
    """Parse training file."""
    examples = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Split by "User:" to get individual examples
    sections = content.split("User: ")[1:]  # Skip first empty section
    
    for section in sections:
        if "Assistant:" in section:
            parts = section.split("Assistant: ", 1)
            if len(parts) == 2:
                user_text = parts[0].strip()
                assistant_text = parts[1].strip()
                
                # Clean up any extra whitespace
                user_text = " ".join(user_text.split())
                assistant_text = " ".join(assistant_text.split())
                
                if user_text and assistant_text:
                    examples.append(WorldModelExample(
                        user=user_text,
                        assistant=assistant_text
                    ))
    
    return examples

def setup_model_and_tokenizer(model_name: str):
    """Setup model and tokenizer with OPTIMIZATION 1: FP16."""
    print(f"Loading model: {model_name}")
    
    # OPTIMIZATION 1: Load model in FP16 from start
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        torch_dtype=torch.float16,  # üöÄ CRITICAL: FP16 for 2x speedup
        low_cpu_mem_usage=True,
        device_map={"": 0}
    )
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    
    # OPTIMIZATION 2: Disable gradient checkpointing for small models
    if hasattr(model, 'gradient_checkpointing_disable'):
        model.gradient_checkpointing_disable()
        print("‚úÖ Gradient checkpointing: DISABLED (30-40% speedup)")
    
    # Set padding token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        
    # Verify FP16
    param_dtype = next(model.parameters()).dtype
    print(f"‚úÖ Model dtype: {param_dtype}")
    if param_dtype != torch.float16:
        print("‚ö†Ô∏è  WARNING: Model not in FP16! Performance will be poor.")
    
    return model, tokenizer

class OptimizedTrainerCallback(TrainerCallback):
    """Monitor performance during training."""
    
    def __init__(self):
        self.monitor = PerformanceMonitor()
        
    def on_step_begin(self, args, state, control, **kwargs):
        self.monitor.start_iteration()
        
    def on_step_end(self, args, state, control, **kwargs):
        self.monitor.end_iteration(
            args.per_device_train_batch_size * args.gradient_accumulation_steps,
            400  # max_length
        )

def main():
    """Main training function."""
    
    # Configuration
    MODEL_NAME = "/home/bigattichouse/workspace/model/Qwen3-0.6B"
    DATA_FILE = "/home/bigattichouse/workspace/worldmodel/data/worldmodel_training_combined.txt"
    OUTPUT_DIR = "./worldmodel_rocm_output_fast"
    
    print("üî• WorldModel FAST Training for ROCm MI50")
    print("=" * 60)
    
    # Create output directory
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Load data
    print(f"Loading training data from: {DATA_FILE}")
    examples = parse_training_file(DATA_FILE)
    print(f"‚úÖ Loaded {len(examples)} valid examples")
    
    if len(examples) == 0:
        print("‚ùå No examples found!")
        return
    
    # Show sample
    print(f"\nüìù Sample example:")
    sample = examples[0]
    print(f"   User: {sample.user[:80]}...")
    print(f"   Assistant: {sample.assistant[:80]}...")
    
    # Split data
    train_size = int(0.9 * len(examples))
    train_examples = examples[:train_size]
    eval_examples = examples[train_size:]
    
    print(f"\nüìä Data split:")
    print(f"   Training: {len(train_examples)} examples")
    print(f"   Evaluation: {len(eval_examples)} examples")
    
    # Load model
    model, tokenizer = setup_model_and_tokenizer(MODEL_NAME)
    
    print(f"‚úÖ Model loaded:")
    print(f"   Parameters: {model.num_parameters():,}")
    print(f"   Vocab size: {tokenizer.vocab_size:,}")
    
    # Create datasets
    print(f"\nüîÑ Creating datasets...")
    train_dataset = WorldModelDataset(train_examples, tokenizer, max_length=400)  # Optimized length
    eval_dataset = WorldModelDataset(eval_examples, tokenizer, max_length=400)
    
    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # Causal LM
        return_tensors="pt"
    )
    
    # OPTIMIZED Training arguments
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=30,
        
        # OPTIMIZATION 3: Increased batch size to utilize 32GB VRAM
        per_device_train_batch_size=4,      # 2x increase from original
        per_device_eval_batch_size=4,       # Match training
        gradient_accumulation_steps=1,       # Direct batching, no accumulation
        
        learning_rate=1e-4,
        warmup_steps=100,
        weight_decay=0.01,
        
        # Performance monitoring
        logging_steps=10,
        eval_steps=100,
        save_steps=200,
        
        eval_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        
        # OPTIMIZATION 5: Parallel data loading
        dataloader_num_workers=4,           # Parallel loading
        dataloader_pin_memory=True,         # Memory efficiency  
        dataloader_persistent_workers=True, # Reuse workers
        
        # OPTIMIZATION 1: FP16 training (ROCm-safe settings)
        fp16=True,                          # Enable mixed precision
        fp16_opt_level="O1",               # Conservative level for ROCm
        fp16_backend="amp",                # Use automatic mixed precision
        fp16_full_eval=False,              # Safer evaluation
        
        # Disable features that slow down training
        gradient_checkpointing=False,       # Disabled for speed
        dataloader_drop_last=True,         # Consistent batch sizes
        
        # ROCm compatibility
        report_to=None,                    # Disable wandb/tensorboard
        push_to_hub=False,
    )
    
    # Performance callback
    performance_callback = OptimizedTrainerCallback()
    
    # Create trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        callbacks=[performance_callback],
        processing_class=tokenizer,
    )
    
    print(f"\nüöÄ Starting OPTIMIZED training...")
    print(f"   Device: {device}")
    print(f"   Model dtype: {next(model.parameters()).dtype}")
    print(f"   Batch size: {training_args.per_device_train_batch_size}")
    print(f"   Effective batch: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
    print(f"   Sequence length: 400 (optimized)")
    print(f"   Data workers: {training_args.dataloader_num_workers}")
    print(f"   FP16 enabled: {training_args.fp16}")
    print(f"   Gradient checkpointing: {training_args.gradient_checkpointing}")
    print(f"   Learning rate: {training_args.learning_rate}")
    print(f"   Epochs: {training_args.num_train_epochs}")
    
    # Verify memory before training
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print(f"\nüíæ Memory status:")
        print(f"   Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        print(f"   Available: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1e9:.1f}GB")
    
    # Training
    start_time = time.time()
    trainer.train()
    training_time = time.time() - start_time
    
    print(f"\n‚úÖ Training completed in {training_time/3600:.2f} hours")
    print(f"üìä Average iteration time: {sum(performance_callback.monitor.iteration_times)/len(performance_callback.monitor.iteration_times):.2f}s")
    
    # Save the model
    print(f"\nüíæ Saving model to {OUTPUT_DIR}/final_model")
    trainer.save_model(f"{OUTPUT_DIR}/final_model")
    tokenizer.save_pretrained(f"{OUTPUT_DIR}/final_model")
    
    print(f"‚úÖ Training completed successfully!")
    print(f"   Model saved to: {OUTPUT_DIR}/final_model")
    print(f"   Use: python3 run_worldmodel_inference.py --model {OUTPUT_DIR}/final_model")

if __name__ == "__main__":
    main()