{"id": "blueprint_tech_data_161", "category": "data_processing", "difficulty": "intermediate", "user_query": "ETL pipeline orchestrator", "response": "<thinking>\nPROBLEM UNDERSTANDING:\n- A system to orchestrate Extract, Transform, Load (ETL) pipelines.\n- It needs to define data sources, transformations, and destinations.\n- It should manage the execution order of jobs, handle failures, and provide monitoring.\n\nSTRATEGIC APPROACH:\n- Design an `EtlOrchestratorService`.\n- Define `EtlJob`, `DataSource`, `Transformation`, and `DataSink` entities.\n- Implement methods for defining jobs, triggering their execution, and monitoring status.\n\nDESIGN PREPARATION:\n- `EtlJob` data model (jobId, name, schedule, steps: array[EtlStep], status: Scheduled/Running/Completed/Failed).\n- `EtlStep` data model (type: Extract/Transform/Load, config: map<string, any>).\n- `EtlOrchestratorService` with methods: `defineEtlJob`, `triggerJob`, `getJobStatus`.\n- `EtlJobRepository` for persistence.\n- Integration with `SchedulerService` (for timed jobs) and `MonitoringService`.\n- Error handling for `JobNotFound`, `InvalidJobDefinition`, `StepExecutionFailed`.\n</thinking>\n\n<blueprint>\nService EtlOrchestratorService {\n  description: \"Orchestrates Extract, Transform, Load (ETL) data pipelines.\",\n  dependencies: {\n    jobRepo: EtlJobRepository,\n    executionEngine: EtlExecutionEngine,\n    scheduler: SchedulerService,\n    notification: NotificationService\n  },\n  methods: {\n    defineEtlJob(name: string, description: string, schedule: string, steps: array[EtlStep]) -> EtlJob {\n      preconditions: [name is unique, steps are validly defined],\n      postconditions: [a new ETL job definition is created with 'Scheduled' status],\n      errors: [InvalidJobDefinition, DuplicateJobName]\n    },\n    triggerJob(jobId: UUID) -> EtlJobRun {\n      preconditions: [jobId exists],\n      postconditions: [the specified ETL job is started, a job run record is created],\n      errors: [EtlJobNotFound, JobAlreadyRunning]\n    },\n    getJobStatus(jobRunId: UUID) -> EtlJobRunStatus {\n      preconditions: [jobRunId exists],\n      postconditions: [returns the current status and progress of the ETL job run],\n      errors: [EtlJobRunNotFound]\n    },\n    cancelJobRun(jobRunId: UUID) -> EtlJobRun {\n      preconditions: [jobRunId exists, status is 'Running' or 'Pending'],\n      postconditions: [the ETL job run is stopped and status updated to 'Canceled'],\n      errors: [EtlJobRunNotFound, JobNotActive]\n    }\n  }\n}\n\nService EtlJobRepository {\n  methods: {\n    save(job: EtlJob) -> EtlJob;\n    findById(jobId: UUID) -> EtlJob;\n  }\n}\n\nService EtlExecutionEngine {\n  methods: {\n    executeStep(step: EtlStep, jobRunId: UUID) -> EtlStepResult;\n  }\n}\n\nType EtlStepType {\n  enum: [EXTRACT, TRANSFORM, LOAD]\n}\n\nType EtlJobStatus {\n  enum: [SCHEDULED, RUNNING, COMPLETED, FAILED, CANCELED]\n}\n\nType EtlStep {\n  id: UUID,\n  type: EtlStepType,\n  name: string,\n  config: map<string, any>\n}\n\nType EtlJob {\n  id: UUID,\n  name: string,\n  description: string,\n  schedule: string, // e.g., cron expression\n  steps: array[EtlStep],\n  status: EtlJobStatus,\n  createdAt: timestamp\n}\n\nType EtlJobRunStatus {\n  enum: [PENDING, RUNNING, COMPLETED, FAILED, CANCELED]\n}\n\nType EtlJobRun {\n  id: UUID,\n  jobId: UUID,\n  startTime: timestamp,\n  endTime: timestamp | null,\n  status: EtlJobRunStatus,\n  stepResults: array[EtlStepResult]\n}\n\nType EtlStepResult {\n  stepId: UUID,\n  status: EtlJobRunStatus,\n  durationSeconds: int,\n  outputDataInfo: map<string, any> // e.g., rows_processed, file_size\n}\n</blueprint>"}